import os
import time
import re
import logging
import requests
import io
import psutil
import threading
import subprocess
import nltk
from flask import Flask, render_template, request, jsonify
from nltk.sentiment import SentimentIntensityAnalyzer
from langdetect import detect
from cachetools import TTLCache, cached
import emoji
from llama_cpp import Llama
from huggingface_hub import hf_hub_download
from duckduckgo_search import DDGS
from PIL import Image, ExifTags

# Configura√ß√µes iniciais do NLTK (necess√°rio apenas na primeira execu√ß√£o)
nltk.download('punkt')
nltk.download('vader_lexicon')

# Configura√ß√µes gerais e logger
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
sentiment_analyzer = SentimentIntensityAnalyzer()
cache = TTLCache(maxsize=500, ttl=3600)

LANGUAGE_MAP = {
    'Portugu√™s': {'code': 'pt-BR', 'instruction': 'Responda em portugu√™s brasileiro'},
    'English': {'code': 'en-US', 'instruction': 'Respond in English'},
    'Espa√±ol': {'code': 'es-ES', 'instruction': 'Responde en espa√±ol'},
    'Fran√ßais': {'code': 'fr-FR', 'instruction': 'R√©ponds en fran√ßais'},
    'Deutsch': {'code': 'de-DE', 'instruction': 'Antworte auf Deutsch'}
}

DEFAULT_MODEL_NAME = "TheBloke/Mistral-7B-Instruct-v0.1-GGUF"
DEFAULT_MODEL_FILE = "mistral-7b-instruct-v0.1.Q4_K_M.gguf"
DEFAULT_LOCAL_MODEL_DIR = "models"

app = Flask(__name__)

def download_model() -> None:
    """
    Faz o download do modelo caso n√£o esteja dispon√≠vel localmente.
    """
    try:
        logger.info("‚è¨ Baixando Modelo...")
        hf_hub_download(
            repo_id=DEFAULT_MODEL_NAME,
            filename=DEFAULT_MODEL_FILE,
            local_dir=DEFAULT_LOCAL_MODEL_DIR,
            resume_download=True
        )
    except Exception as e:
        logger.error(f"‚ùå Falha no Download: {e}")
        raise e

def load_model() -> Llama:
    """
    Carrega o modelo neural. Se n√£o existir localmente, faz o download.
    """
    model_path = os.path.join(DEFAULT_LOCAL_MODEL_DIR, DEFAULT_MODEL_FILE)
    if not os.path.exists(model_path):
        download_model()
    try:
        n_gpu_layers = 33 if psutil.virtual_memory().available > 4 * 1024 ** 3 else 15
        model = Llama(
            model_path=model_path,
            n_ctx=4096,
            n_threads=psutil.cpu_count(logical=True),
            n_gpu_layers=n_gpu_layers
        )
        logger.info("ü§ñ Modelo Neural Carregado")
        return model
    except Exception as e:
        logger.error(f"‚ùå Erro na Inicializa√ß√£o do Modelo: {e}")
        raise e

model = load_model()

def build_messages(query: str, lang_config: dict, style: str) -> tuple[list, float]:
    """
    Constr√≥i a lista de mensagens e define a temperatura de acordo com o estilo.
    """
    if style == "T√©cnico":
        system_instruction = f"{lang_config['instruction']}. Seja detalhado e t√©cnico."
        temperature = 0.7
    else:
        system_instruction = f"{lang_config['instruction']}. Responda de forma livre e criativa."
        temperature = 0.9
    messages = [
        {"role": "system", "content": system_instruction},
        {"role": "user", "content": query}
    ]
    return messages, temperature

@cached(cache)
def generate_response(query: str, lang: str, style: str) -> str:
    """
    Gera uma resposta a partir da consulta, utilizando o modelo de linguagem.
    """
    start_time = time.time()
    lang_config = LANGUAGE_MAP.get(lang, LANGUAGE_MAP['Portugu√™s'])
    messages, temperature = build_messages(query, lang_config, style)
    
    try:
        response = model.create_chat_completion(
            messages=messages,
            temperature=temperature,
            max_tokens=800,
            stop=["</s>"]
        )
        raw_response = response['choices'][0]['message']['content']
        final_response = validate_language(raw_response, lang_config)
        logger.info(f"‚úÖ Resposta gerada em {time.time() - start_time:.2f}s")
        return final_response
    except Exception as e:
        logger.error(f"‚ùå Erro ao gerar resposta: {e}")
        return f"Erro ao gerar resposta: {e}"

def validate_language(text: str, lang_config: dict) -> str:
    """
    Verifica se o texto est√° no idioma esperado; se n√£o, corrige a linguagem.
    """
    try:
        detected_lang = detect(text)
        expected_lang = lang_config['code'].split('-')[0]
        if detected_lang != expected_lang:
            logger.info(f"Idioma detectado ({detected_lang}) difere do esperado ({expected_lang}). Corrigindo...")
            return correct_language(text, lang_config)
        return text
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Falha na detec√ß√£o de idioma: {e}. Retornando texto original.")
        return text

def correct_language(text: str, lang_config: dict) -> str:
    """
    Corrige o idioma do texto utilizando o modelo para tradu√ß√£o.
    """
    try:
        correction_prompt = f"Traduza para {lang_config['instruction']}:\n{text}"
        corrected = model.create_chat_completion(
            messages=[{"role": "user", "content": correction_prompt}],
            temperature=0.3,
            max_tokens=1000
        )
        corrected_text = corrected['choices'][0]['message']['content']
        return f"[Traduzido]\n{corrected_text}"
    except Exception as e:
        logger.error(f"‚ùå Erro na corre√ß√£o de idioma: {e}")
        return text

def advanced_forensic_analysis(text: str) -> dict:
    """
    Realiza uma an√°lise forense aprimorada no texto fornecido, extraindo informa√ß√µes relevantes.
    """
    forensic_info = {}
    try:
        # Padr√µes de regex para diversas informa√ß√µes
        ip_pattern = re.compile(r'\b(?:\d{1,3}\.){3}\d{1,3}\b')
        ipv6_pattern = re.compile(r'\b(?:[A-Fa-f0-9]{1,4}:){7}[A-Fa-f0-9]{1,4}\b')
        email_pattern = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}')
        phone_pattern = re.compile(r'\+?\d[\d\s()-]{7,}\d')
        url_pattern = re.compile(r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+')
        mac_pattern = re.compile(r'\b(?:[0-9A-Fa-f]{2}[:-]){5}(?:[0-9A-Fa-f]{2})\b')
        md5_pattern = re.compile(r'\b[a-fA-F0-9]{32}\b')
        sha1_pattern = re.compile(r'\b[a-fA-F0-9]{40}\b')
        sha256_pattern = re.compile(r'\b[a-fA-F0-9]{64}\b')
        cve_pattern = re.compile(r'\bCVE-\d{4}-\d{4,7}\b')
        
        # Extra√ß√£o das informa√ß√µes
        ip_addresses = ip_pattern.findall(text)
        if ip_addresses:
            forensic_info['Endere√ßos IPv4'] = list(set(ip_addresses))
        
        ipv6_addresses = ipv6_pattern.findall(text)
        if ipv6_addresses:
            forensic_info['Endere√ßos IPv6'] = list(set(ipv6_addresses))
        
        emails = email_pattern.findall(text)
        if emails:
            forensic_info['E-mails'] = list(set(emails))
        
        phones = phone_pattern.findall(text)
        if phones:
            forensic_info['Telefones'] = list(set(phones))
        
        urls = url_pattern.findall(text)
        if urls:
            forensic_info['URLs'] = list(set(urls))
        
        macs = mac_pattern.findall(text)
        if macs:
            forensic_info['Endere√ßos MAC'] = list(set(macs))
        
        md5_hashes = md5_pattern.findall(text)
        if md5_hashes:
            forensic_info['Hashes MD5'] = list(set(md5_hashes))
        
        sha1_hashes = sha1_pattern.findall(text)
        if sha1_hashes:
            forensic_info['Hashes SHA1'] = list(set(sha1_hashes))
        
        sha256_hashes = sha256_pattern.findall(text)
        if sha256_hashes:
            forensic_info['Hashes SHA256'] = list(set(sha256_hashes))
        
        cve_ids = cve_pattern.findall(text)
        if cve_ids:
            forensic_info['IDs CVE'] = list(set(cve_ids))
        
    except Exception as e:
        logger.error(f"‚ùå Erro durante a an√°lise forense: {e}")
    
    return forensic_info

def convert_to_degrees(value) -> float:
    """
    Converte coordenadas GPS no formato de fra√ß√µes para graus decimais.
    """
    try:
        d, m, s = value
        degrees = d[0] / d[1]
        minutes = m[0] / m[1] / 60
        seconds = s[0] / s[1] / 3600
        return degrees + minutes + seconds
    except Exception as e:
        logger.error(f"‚ùå Erro na convers√£o de coordenadas: {e}")
        raise e

def analyze_image_metadata(url: str) -> dict:
    """
    Obt√©m e analisa os metadados EXIF de uma imagem a partir de sua URL.
    """
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        image_data = response.content
        image = Image.open(io.BytesIO(image_data))
        exif = image._getexif()
        meta = {}
        if exif:
            for tag_id, value in exif.items():
                tag = Image.ExifTags.TAGS.get(tag_id, tag_id)
                meta[tag] = value
            if "GPSInfo" in meta:
                gps_info = meta["GPSInfo"]
                try:
                    lat = convert_to_degrees(gps_info.get(2))
                    if gps_info.get(1) != "N":
                        lat = -lat
                    lon = convert_to_degrees(gps_info.get(4))
                    if gps_info.get(3) != "E":
                        lon = -lon
                    meta["GPS Coordinates"] = (
                        f"{lat}, {lon} (Google Maps: https://maps.google.com/?q={lat},{lon})"
                    )
                except Exception as e:
                    meta["GPS Extraction Error"] = str(e)
        else:
            meta["info"] = "Nenhum metadado EXIF encontrado."
        return meta
    except Exception as e:
        logger.error(f"‚ùå Erro ao analisar metadados da imagem: {e}")
        return {"error": str(e)}

def process_investigation(target: str, sites_meta: int = 5, investigation_focus: str = "",
                          search_news: bool = False, search_leaked_data: bool = False) -> str:
    """
    Processa uma investiga√ß√£o online com base no alvo informado.
    """
    logger.info(f"üîç Iniciando investiga√ß√£o para: {repr(target)}")
    if not target.strip():
        return "Erro: Por favor, insira um alvo para investiga√ß√£o."
    
    try:
        with DDGS() as ddgs:
            results = ddgs.text(keywords=target, max_results=sites_meta)
    except Exception as e:
        logger.error(f"‚ùå Erro na pesquisa com DDGS: {e}")
        return f"Erro na pesquisa: {e}"
    
    info_msg = f"Apenas {len(results)} sites encontrados para '{target}'.<br>" if len(results) < sites_meta else ""
    formatted_results = "<br>".join(
        f"‚Ä¢ {res.get('title', 'Sem t√≠tulo')}<br>&nbsp;&nbsp;{res.get('href', 'Sem link')}<br>&nbsp;&nbsp;{res.get('body', '')}"
        for res in results
    )
    links_table = (
        "<table border='1' style='width:100%; border-collapse: collapse; text-align: left;'>"
        "<thead><tr><th>T√≠tulo</th><th>Link</th></tr></thead><tbody>"
    )
    for res in results:
        title = res.get('title', 'Sem t√≠tulo')
        href = res.get('href', 'Sem link')
        links_table += f"<tr><td>{title}</td><td><a href='{href}' target='_blank'>{href}</a></td></tr>"
    links_table += "</tbody></table>"
    
    forensic_analysis = advanced_forensic_analysis(formatted_results)
    forensic_details = "<br>".join(f"{k}: {v}" for k, v in forensic_analysis.items() if v)
    
    investigation_prompt = f"Analise os dados obtidos sobre '{target}'"
    if investigation_focus:
        investigation_prompt += f", focando em '{investigation_focus}'"
    investigation_prompt += "<br><br>Resultados de sites:<br>" + formatted_results
    if forensic_details:
        investigation_prompt += "<br><br>An√°lise Forense Extra√≠da:<br>" + forensic_details
    investigation_prompt += "<br><br>Elabore um relat√≥rio detalhado com liga√ß√µes, riscos e informa√ß√µes relevantes."
    
    try:
        investigation_response = model.create_chat_completion(
            messages=[
                {"role": "system", "content": "Voc√™ √© um perito em investiga√ß√£o online e an√°lise forense digital. Seja minucioso e detalhado."},
                {"role": "user", "content": investigation_prompt}
            ],
            temperature=0.7,
            max_tokens=1000,
            stop=["</s>"]
        )
        report = investigation_response['choices'][0]['message']['content']
        final_report = info_msg + "<br>" + report + "<br><br>Links encontrados:<br>" + links_table
        return final_report
    except Exception as e:
        logger.error(f"‚ùå Erro na investiga√ß√£o: {e}")
        return f"Erro na investiga√ß√£o: {e}"

@app.route('/')
def index():
    """
    Renderiza a p√°gina principal.
    """
    return render_template('index.html')

@app.route('/ask', methods=['POST'])
def ask():
    """
    Endpoint que processa as requisi√ß√µes em tr√™s modos:
      - Chat
      - Investiga√ß√£o
      - Metadados
    """
    user_input = request.form.get('user_input', '')
    mode = request.form.get('mode', 'Chat')
    lang = request.form.get('language', 'Portugu√™s')
    style = request.form.get('style', 'T√©cnico')
    
    if mode == "Investiga√ß√£o":
        if not user_input.strip():
            return jsonify({'response': "Erro: Por favor, insira um alvo para investiga√ß√£o."})
        try:
            sites_meta = int(request.form.get('sites_meta', 5))
            investigation_focus = request.form.get('investigation_focus', '')
            search_news = request.form.get('search_news', 'false').lower() == 'true'
            search_leaked_data = request.form.get('search_leaked_data', 'false').lower() == 'true'
            response_text = process_investigation(user_input, sites_meta, investigation_focus, search_news, search_leaked_data)
            return jsonify({'response': response_text})
        except Exception as e:
            logger.error(f"‚ùå Erro no modo Investiga√ß√£o: {e}")
            return jsonify({'error': str(e)}), 500
    elif mode == "Metadados":
        if not user_input.strip():
            return jsonify({'response': "Erro: Por favor, insira um link de imagem."})
        try:
            meta = analyze_image_metadata(user_input)
            formatted_meta = "<br>".join(f"{k}: {v}" for k, v in meta.items())
            return jsonify({'response': formatted_meta})
        except Exception as e:
            logger.error(f"‚ùå Erro no modo Metadados: {e}")
            return jsonify({'error': str(e)}), 500
    else:  # Modo Chat
        try:
            response_text = generate_response(user_input, lang, style)
            return jsonify({'response': response_text})
        except Exception as e:
            logger.error(f"‚ùå Erro no modo Chat: {e}")
            return jsonify({'error': str(e)}), 500

@app.errorhandler(404)
def not_found(e):
    return jsonify({'error': 'Endpoint not found'}), 404

@app.errorhandler(500)
def internal_error(e):
    return jsonify({'error': 'Internal server error'}), 500

def ensure_localtunnel_installed():
    """
    Verifica se o LocalTunnel est√° instalado; caso n√£o esteja, instala o Node.js, npm e localtunnel.
    """
    try:
        # Verifica se o comando "lt" existe
        result = subprocess.run("which lt", shell=True, capture_output=True, text=True)
        if not result.stdout.strip():
            print("LocalTunnel n√£o encontrado. Instalando Node.js, npm e localtunnel...")
            # Instala Node.js e npm (no Colab, geralmente j√° h√° apt-get dispon√≠vel)
            subprocess.run("apt-get install -y nodejs npm", shell=True, check=True)
            # Instala o localtunnel globalmente via npm
            subprocess.run("npm install -g localtunnel", shell=True, check=True)
            print("LocalTunnel instalado com sucesso!")
        else:
            print("LocalTunnel j√° est√° instalado.")
    except Exception as e:
        print("Erro ao verificar ou instalar LocalTunnel:", e)

if __name__ == '__main__':
    # Verifica e instala o LocalTunnel, se necess√°rio
    ensure_localtunnel_installed()

    # Inicia o LocalTunnel para expor a porta 5000.
    def read_lt_output(process):
        for line in process.stdout:
            print("LocalTunnel:", line.strip())

    lt_command = "lt --port 5000"
    print("Iniciando LocalTunnel para expor a aplica√ß√£o na porta 5000...")
    lt_process = subprocess.Popen(lt_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    
    # Thread para exibir a sa√≠da do LocalTunnel (onde o URL p√∫blico aparecer√°)
    lt_thread = threading.Thread(target=read_lt_output, args=(lt_process,), daemon=True)
    lt_thread.start()

    print("Executando a aplica√ß√£o Flask...")
    app.run(host='0.0.0.0', port=5000, debug=True)
