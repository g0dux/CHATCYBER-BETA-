import sys
# Patch para contornar a aus√™ncia do m√≥dulo 'distutils'
try:
    import distutils
except ImportError:
    try:
        import setuptools._distutils as distutils
        sys.modules['distutils'] = distutils
    except ImportError:
        pass

import os
import time
import re
import logging
import requests
import io
import psutil
import threading
import subprocess
import nltk
import concurrent.futures
import email
from email import policy
from email.parser import BytesParser
import numpy as np
from sklearn.ensemble import IsolationForest
from llama_cpp import Llama
from huggingface_hub import hf_hub_download
from duckduckgo_search import DDGS
from PIL import Image, ExifTags
import tempfile
import multiprocessing
import socket  # Necess√°rio para descoberta de IP
import gradio as gr  # Integra√ß√£o com Gradio.live

# Tenta importar pyshark para an√°lise de tr√°fego de rede
try:
    import pyshark
except ImportError:
    pyshark = None
    logging.warning("pyshark n√£o est√° instalado. A funcionalidade de an√°lise de rede n√£o estar√° dispon√≠vel.")

# Configura√ß√µes iniciais do NLTK (necess√°rio apenas na primeira execu√ß√£o)
nltk.download('punkt')
nltk.download('vader_lexicon')

# Configura√ß√µes gerais e logger
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
from nltk.sentiment import SentimentIntensityAnalyzer
sentiment_analyzer = SentimentIntensityAnalyzer()

LANGUAGE_MAP = {
    'Portugu√™s': {'code': 'pt-BR', 'instruction': 'Responda em portugu√™s brasileiro'},
    'English': {'code': 'en-US', 'instruction': 'Respond in English'},
    'Espa√±ol': {'code': 'es-ES', 'instruction': 'Responde en espa√±ol'},
    'Fran√ßais': {'code': 'fr-FR', 'instruction': 'R√©ponds en fran√ßais'},
    'Deutsch': {'code': 'de-DE', 'instruction': 'Antworte auf Deutsch'}
}

DEFAULT_MODEL_NAME = "TheBloke/Mistral-7B-Instruct-v0.1-GGUF"
DEFAULT_MODEL_FILE = "mistral-7b-instruct-v0.1.Q4_K_M.gguf"
DEFAULT_LOCAL_MODEL_DIR = "models"

# ===== Cache simples em mem√≥ria =====
cache = {}

def get_cached_response(query: str, lang: str, style: str) -> str:
    key = f"response:{query}:{lang}:{style}"
    return cache.get(key)

def set_cached_response(query: str, lang: str, style: str, response_text: str, ttl: int = 3600) -> None:
    key = f"response:{query}:{lang}:{style}"
    cache[key] = response_text  # TTL n√£o implementado nesta vers√£o

# ===== COMPILED_REGEX_PATTERNS =====
COMPILED_REGEX_PATTERNS = {
    'ip': re.compile(r'\b(?:\d{1,3}\.){3}\d{1,3}\b'),
    'ipv6': re.compile(r'\b(?:[A-Fa-f0-9]{1,4}:){7}[A-Fa-f0-9]{1,4}\b'),
    'email': re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'),
    'phone': re.compile(r'\+?\d[\d\s()-]{7,}\d'),
    'url': re.compile(r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+'),
    'mac': re.compile(r'\b(?:[0-9A-Fa-f]{2}[:-]){5}(?:[0-9A-Fa-f]{2})\b'),
    'md5': re.compile(r'\b[a-fA-F0-9]{32}\b'),
    'sha1': re.compile(r'\b[a-fA-F0-9]{40}\b'),
    'sha256': re.compile(r'\b[a-fA-F0-9]{64}\b'),
    'cve': re.compile(r'\bCVE-\d{4}-\d{4,7}\b'),
    'imei': re.compile(r'\b\d{15}\b'),
    'cpf': re.compile(r'\b\d{3}\.\d{3}\.\d{3}-\d{2}\b'),
    'cnpj': re.compile(r'\b\d{2}\.\d{3}\.\d{3}/\d{4}-\d{2}\b'),
    'ssn': re.compile(r'\b\d{3}-\d{2}-\d{4}\b'),
    'uuid': re.compile(r'\b[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}\b'),
    'cc': re.compile(r'\b(?:\d[ -]*?){13,16}\b'),
    'btc': re.compile(r'\b(?:[13][a-km-zA-HJ-NP-Z1-9]{25,34})\b'),
    'ethereum': re.compile(r'\b0x[a-fA-F0-9]{40}\b'),
    'jwt': re.compile(r'\beyJ[a-zA-Z0-9-_]+?\.[a-zA-Z0-9-_]+?\.[a-zA-Z0-9-_]+?\b'),
    'cidr': re.compile(r'\b(?:\d{1,3}\.){3}\d{1,3}/\d{1,2}\b'),
    'iso8601': re.compile(r'\d{4}-\d{2}-\d{2}[ T]\d{2}:\d{2}:\d{2}'),
    'sha512': re.compile(r'\b[a-fA-F0-9]{128}\b'),
    'base64': re.compile(r'\b(?:[A-Za-z0-9+/]{4})*(?:[A-Za-z0-9+/]{2}==|[A-Za-z0-9+/]{3}=)?\b'),
    'google_api_key': re.compile(r'\bAIza[0-9A-Za-z-_]{35}\b'),
    'iban': re.compile(r'\b[A-Z]{2}\d{2}[A-Z0-9]{11,30}\b'),
    'us_phone': re.compile(r'\(?\b\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}\b'),
    'twitter_handle': re.compile(r'@\w{1,15}\b'),
    'date_mmddyyyy': re.compile(r'\b(0?[1-9]|1[0-2])/(0?[1-9]|[12]\d|3[01])/\d{4}\b'),
    'win_path': re.compile(r'\b[a-zA-Z]:\\(?:[^\\\/:*?"<>|\r\n]+\\)*[^\\\/:*?"<>|\r\n]+\b'),
    'ipv4_port': re.compile(r'\b(?:\d{1,3}\.){3}\d{1,3}:\d+\b'),
    'http_status': re.compile(r'HTTP/\d\.\d"\s(\d{3})\b'),
    'version': re.compile(r'\b\d+\.\d+(\.\d+)?\b'),
    'bitcoin_wif': re.compile(r'\b[5KL][1-9A-HJ-NP-Za-km-z]{50,51}\b'),
    'github_repo': re.compile(r'https?://(?:www\.)?github\.com/[A-Za-z0-9_.-]+/[A-Za-z0-9_.-]+'),
    'sql_injection': re.compile(r"(?i)\b(SELECT|INSERT|UPDATE|DELETE|DROP|ALTER|CREATE|EXEC)\b"),
    'unix_path': re.compile(r'(/(?:[\w._-]+/)*[\w._-]+)'),
    'base32': re.compile(r'\b(?:[A-Z2-7]{8,})\b'),
    'bitcoin_cash': re.compile(r'\b(?:q|p)[a-z0-9]{41}\b'),
    'passport': re.compile(r'\b\d{9}\b'),
    'win_registry': re.compile(r'(?:HKEY_LOCAL_MACHINE|HKEY_CURRENT_USER|HKEY_CLASSES_ROOT|HKEY_USERS|HKEY_CURRENT_CONFIG)\\[\\\w]+'),
    'onion_v2': re.compile(r'\b[a-z2-7]{16}\.onion\b'),
    'onion_v3': re.compile(r'\b[a-z2-7]{56}\.onion\b'),
    'domain': re.compile(r'\b(?:[a-zA-Z0-9-]+\.)+[a-zA-Z]{2,}\b'),
    'e164_phone': re.compile(r'\+\d{10,15}'),
    'geo_coordinates': re.compile(r'[-+]?([1-8]?\d(\.\d+)?|90(\.0+)?)[, ]+\s*[-+]?((1[0-7]\d)|([1-9]?\d))(\.\d+)?')
}

# ===== Fun√ß√µes de Download e Carregamento do Modelo =====
def download_model() -> None:
    try:
        logger.info("‚è¨ Baixando Modelo...")
        hf_hub_download(
            repo_id=DEFAULT_MODEL_NAME,
            filename=DEFAULT_MODEL_FILE,
            local_dir=DEFAULT_LOCAL_MODEL_DIR,
            resume_download=True
        )
    except Exception as e:
        logger.error(f"‚ùå Falha no Download: {e}")
        raise e

def load_model() -> Llama:
    model_path = os.path.join(DEFAULT_LOCAL_MODEL_DIR, DEFAULT_MODEL_FILE)
    if not os.path.exists(model_path):
        download_model()
    try:
        n_gpu_layers = 15
        n_batch = 512
        try:
            import GPUtil
            gpus = GPUtil.getGPUs()
            if gpus:
                n_gpu_layers = -1
                n_batch = 1024
                logger.info("GPU detectada. Utilizando todas as camadas na GPU (n_gpu_layers=-1) e n_batch=1024.")
            else:
                logger.info("Nenhuma GPU detectada. Usando configura√ß√£o otimizada para CPU.")
        except Exception as gpu_error:
            logger.warning(f"Erro na detec√ß√£o da GPU: {gpu_error}. Configura√ß√£o para CPU ser√° utilizada.")
        model = Llama(
            model_path=model_path,
            n_ctx=4096,
            n_threads=psutil.cpu_count(logical=True),
            n_gpu_layers=n_gpu_layers,
            n_batch=n_batch
        )
        logger.info(f"ü§ñ Modelo Neural Carregado com n_gpu_layers={n_gpu_layers}, n_batch={n_batch} e n_threads={psutil.cpu_count(logical=True)}")
        return model
    except Exception as e:
        logger.error(f"‚ùå Erro na Inicializa√ß√£o do Modelo: {e}")
        raise e

model = load_model()

# ===== Fun√ß√µes para Gera√ß√£o de Resposta e Valida√ß√£o de Idioma =====
def build_messages(query: str, lang_config: dict, style: str) -> tuple[list, float]:
    if style == "T√©cnico":
        system_instruction = f"{lang_config['instruction']}. Seja detalhado e t√©cnico."
        temperature = 0.7
    else:
        system_instruction = f"{lang_config['instruction']}. Responda de forma livre e criativa."
        temperature = 0.9
    messages = [
        {"role": "system", "content": system_instruction},
        {"role": "user", "content": query}
    ]
    return messages, temperature

def generate_response(query: str, lang: str, style: str) -> str:
    start_time = time.time()
    cached_text = get_cached_response(query, lang, style)
    if cached_text:
        logger.info(f"‚úÖ Resposta obtida do cache em {time.time() - start_time:.2f}s")
        return cached_text
    lang_config = LANGUAGE_MAP.get(lang, LANGUAGE_MAP['Portugu√™s'])
    messages, temperature = build_messages(query, lang_config, style)
    try:
        response = model.create_chat_completion(
            messages=messages,
            temperature=temperature,
            max_tokens=800,
            stop=["</s>"]
        )
        raw_response = response['choices'][0]['message']['content']
        final_response = validate_language(raw_response, lang_config)
        logger.info(f"‚úÖ Resposta gerada em {time.time() - start_time:.2f}s")
        set_cached_response(query, lang, style, final_response)
        return final_response
    except Exception as e:
        logger.error(f"‚ùå Erro ao gerar resposta: {e}")
        return f"Erro ao gerar resposta: {e}"

def validate_language(text: str, lang_config: dict) -> str:
    try:
        from langdetect import detect
        detected_lang = detect(text)
        expected_lang = lang_config['code'].split('-')[0]
        if detected_lang != expected_lang:
            logger.info(f"Idioma detectado ({detected_lang}) difere do esperado ({expected_lang}). Corrigindo...")
            return correct_language(text, lang_config)
        return text
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Falha na detec√ß√£o de idioma: {e}. Retornando texto original.")
        return text

def correct_language(text: str, lang_config: dict) -> str:
    try:
        correction_prompt = f"Traduza para {lang_config['instruction']}:\n{text}"
        corrected = model.create_chat_completion(
            messages=[{"role": "user", "content": correction_prompt}],
            temperature=0.3,
            max_tokens=1000
        )
        corrected_text = corrected['choices'][0]['message']['content']
        return f"[Traduzido]\n{corrected_text}"
    except Exception as e:
        logger.error(f"‚ùå Erro na corre√ß√£o de idioma: {e}")
        return text

# ===== Fun√ß√µes de An√°lise Forense e Processamento de Texto =====
def advanced_forensic_analysis(text: str) -> dict:
    forensic_info = {}
    try:
        for key, pattern in COMPILED_REGEX_PATTERNS.items():
            matches = pattern.findall(text)
            if matches:
                forensic_info[key] = list(set(matches))
    except Exception as e:
        logger.error(f"‚ùå Erro durante a an√°lise forense: {e}")
    return forensic_info

def convert_to_degrees(value) -> float:
    try:
        d, m, s = value
        degrees = d[0] / d[1]
        minutes = m[0] / m[1] / 60
        seconds = s[0] / s[1] / 3600
        return degrees + minutes + seconds
    except Exception as e:
        logger.error(f"‚ùå Erro na convers√£o de coordenadas: {e}")
        raise e

def analyze_image_metadata(url: str) -> dict:
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        image_data = response.content
        image = Image.open(io.BytesIO(image_data))
        exif = image._getexif()
        meta = {}
        if exif:
            for tag_id, value in exif.items():
                tag = Image.ExifTags.TAGS.get(tag_id, tag_id)
                meta[tag] = value
            if "GPSInfo" in meta:
                gps_info = meta["GPSInfo"]
                try:
                    lat = convert_to_degrees(gps_info.get(2))
                    if gps_info.get(1) != "N":
                        lat = -lat
                    lon = convert_to_degrees(gps_info.get(4))
                    if gps_info.get(3) != "E":
                        lon = -lon
                    meta["GPS Coordinates"] = f"{lat}, {lon} (Google Maps: https://maps.google.com/?q={lat},{lon})"
                except Exception as e:
                    meta["GPS Extraction Error"] = str(e)
        else:
            meta["info"] = "Nenhum metadado EXIF encontrado."
        return meta
    except Exception as e:
        logger.error(f"‚ùå Erro ao analisar metadados da imagem: {e}")
        return {"error": str(e)}

# ===== Funcionalidades de Investiga√ß√£o Online =====
def perform_search(query: str, search_type: str, max_results: int) -> list:
    try:
        with DDGS() as ddgs:
            if search_type == 'web':
                return list(ddgs.text(keywords=query, max_results=max_results))
            elif search_type == 'news':
                return list(ddgs.news(keywords=query, max_results=max_results))
            elif search_type == 'leaked':
                return list(ddgs.text(keywords=f"{query} leaked", max_results=max_results))
            else:
                return []
    except Exception as e:
        logger.error(f"Erro na busca ({search_type}): {e}")
        return []

def format_search_results(results: list, section_title: str) -> tuple:
    count = len(results)
    info_message = f"Apenas {count} resultados encontrados para '{section_title}'.<br>" if count < 1 else ""
    formatted_text = "<br>".join(
        f"‚Ä¢ {res.get('title', 'Sem t√≠tulo')}<br>&nbsp;&nbsp;{res.get('href', 'Sem link')}<br>&nbsp;&nbsp;{res.get('body', '')}"
        for res in results
    )
    links_table = (
        f"<h3>{section_title}</h3>"
        "<table border='1' style='width:100%; border-collapse: collapse; text-align: left;'>"
        "<thead><tr><th>N¬∫</th><th>T√≠tulo</th><th>Link</th></tr></thead><tbody>"
    )
    for i, res in enumerate(results, 1):
        title = res.get('title', 'Sem t√≠tulo')
        href = res.get('href', 'Sem link')
        links_table += f"<tr><td>{i}</td><td>{title}</td><td><a href='{href}' target='_blank'>{href}</a></td></tr>"
    links_table += "</tbody></table>"
    return formatted_text, links_table, info_message

def process_investigation(target: str, sites_meta: int = 5, investigation_focus: str = "",
                          search_news: bool = False, search_leaked_data: bool = False) -> tuple:
    logger.info(f"üîç Iniciando investiga√ß√£o para: {repr(target)}")
    if not target.strip():
        return "Erro: Por favor, insira um alvo para investiga√ß√£o.", ""
    
    search_tasks = {}
    with concurrent.futures.ThreadPoolExecutor() as executor:
        search_tasks['web'] = executor.submit(perform_search, target, 'web', sites_meta)
        if search_news:
            search_tasks['news'] = executor.submit(perform_search, target, 'news', sites_meta)
        if search_leaked_data:
            search_tasks['leaked'] = executor.submit(perform_search, target, 'leaked', sites_meta)
    
    results_web = search_tasks['web'].result() if 'web' in search_tasks else []
    results_news = search_tasks['news'].result() if 'news' in search_tasks else []
    results_leaked = search_tasks['leaked'].result() if 'leaked' in search_tasks else []
    
    formatted_web, links_web, info_web = format_search_results(results_web, "Sites")
    formatted_news, links_news, info_news = ("", "", "") if not results_news else format_search_results(results_news, "Not√≠cias")
    formatted_leaked, links_leaked, info_leaked = ("", "", "") if not results_leaked else format_search_results(results_leaked, "Dados Vazados")
    
    combined_results_text = ""
    if formatted_web:
        combined_results_text += "<br><br>Resultados de Sites:<br>" + formatted_web
    if formatted_news:
        combined_results_text += "<br><br>Not√≠cias:<br>" + formatted_news
    if formatted_leaked:
        combined_results_text += "<br><br>Dados Vazados:<br>" + formatted_leaked
    
    forensic_analysis = advanced_forensic_analysis(combined_results_text)
    forensic_details = "<br>".join(f"{k}: {v}" for k, v in forensic_analysis.items() if v)
    
    investigation_prompt = f"Analise os dados obtidos sobre '{target}'"
    if investigation_focus:
        investigation_prompt += f", focando em '{investigation_focus}'"
    investigation_prompt += "<br>" + combined_results_text
    if forensic_details:
        investigation_prompt += "<br><br>An√°lise Forense Extra√≠da:<br>" + forensic_details
    investigation_prompt += "<br><br>Elabore um relat√≥rio detalhado com liga√ß√µes, riscos e informa√ß√µes relevantes."
    
    try:
        investigation_response = model.create_chat_completion(
            messages=[
                {"role": "system", "content": "Voc√™ √© um perito policial e forense digital, experiente em m√©todos policiais de investiga√ß√£o. Utilize t√©cnicas de an√°lise de evid√™ncias, protocolos forenses e investiga√ß√£o digital para identificar padr√µes, rastrear conex√µes e coletar evid√™ncias relevantes. Seja minucioso, preciso e detalhado."},
                {"role": "user", "content": investigation_prompt}
            ],
            temperature=0.7,
            max_tokens=1000,
            stop=["</s>"]
        )
        report = investigation_response['choices'][0]['message']['content']
        links_combined = links_web + (links_news if links_news else "") + (links_leaked if links_leaked else "")
        return report, links_combined
    except Exception as e:
        logger.error(f"‚ùå Erro na investiga√ß√£o: {e}")
        return f"Erro na investiga√ß√£o: {e}", ""

# ===== Integra√ß√£o com Gradio.live (Interface Aprimorada) =====
def gradio_interface(query, mode, language, style, investigation_focus, num_sites, search_news, search_leaked_data):
    if mode == "Chat":
        result = generate_response(query, language, style)
        return result, ""  # Segundo campo vazio para links
    elif mode == "Investiga√ß√£o":
        sites_meta = int(num_sites)
        report, links_table = process_investigation(query, sites_meta, investigation_focus,
                                                     search_news, search_leaked_data)
        return report, links_table
    else:
        return "Modo n√£o suportado.", ""

def build_gradio_interface():
    with gr.Blocks(title="Interface de IA - Chat & Investiga√ß√£o") as demo:
        gr.Markdown("# Interface de IA - Chat & Investiga√ß√£o")
        gr.Markdown("### Insira os par√¢metros para interagir com a IA")
        with gr.Row():
            with gr.Column(scale=1):
                query_input = gr.Textbox(label="Pergunta/Alvo", placeholder="Digite sua pergunta (Chat) ou o alvo (Investiga√ß√£o)...", lines=3)
                mode_input = gr.Radio(["Chat", "Investiga√ß√£o"], label="Modo", value="Chat", interactive=True)
                language_input = gr.Radio(["Portugu√™s", "English", "Espa√±ol", "Fran√ßais", "Deutsch"], label="Idioma", value="Portugu√™s", interactive=True)
                style_input = gr.Radio(["T√©cnico", "Criativo"], label="Estilo", value="T√©cnico", interactive=True)
                investigation_focus = gr.Textbox(label="Foco da Investiga√ß√£o (opcional)", placeholder="Ex: vulnerabilidades, evid√™ncias, etc.", lines=1)
                num_sites = gr.Number(label="N√∫mero de Sites", value=5, precision=0)
                search_news = gr.Checkbox(label="Pesquisar Not√≠cias", value=False)
                search_leaked_data = gr.Checkbox(label="Pesquisar Dados Vazados", value=False)
                submit_btn = gr.Button("Enviar")
            with gr.Column(scale=1):
                report_output = gr.HTML(label="Relat√≥rio")
                links_output = gr.HTML(label="Links Encontrados")
        submit_btn.click(gradio_interface, inputs=[query_input, mode_input, language_input, style_input,
                                                     investigation_focus, num_sites, search_news, search_leaked_data],
                         outputs=[report_output, links_output])
    return demo

demo = build_gradio_interface()

def launch_gradio():
    print("Iniciando Gradio (o sistema escolher√° uma porta livre).")
    demo.launch(share=True)

# ===== Execu√ß√£o da Aplica√ß√£o =====
if __name__ == '__main__':
    launch_gradio()
